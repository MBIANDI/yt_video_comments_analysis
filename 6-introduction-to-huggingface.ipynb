{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid blue; padding: 5px; color: red; text-align: center; font-size: 24px;\">\n",
    " Hugging Face Ecosystem\n",
    "</div>\n",
    "\n",
    "Applying a novel machine learning architecture to a new task can be a complex undertaking, and usually involves the following steps:\n",
    "1. Implement new code for the model architecture (Pytorch or TensorFlow)\n",
    "2. Load the pretrained weights from a server if they are available\n",
    "3. Preprocess the inputs, pass them throught the model, and apply some task-specific postprocessing\n",
    "4. Implement dataloaders and define loss functions and optimizers to train the model\n",
    "\n",
    "Each of theses steps can be time consuming and load pretrained weights can be very hard if the realesed code is not standardized.\n",
    "\n",
    "<span style=\"color:red\">Hugging Face comes to the NLP practitionner's rescue. So what is Hugging Face?</span>\n",
    "\n",
    "Hugging Face is a company that focuses on natural language processing and provides various tools and libraries for working with NLP. The Hugging Face ecosystem consists of mainly two parts:\n",
    "- `a family of librairies`\n",
    "- `The Hub`\n",
    "<center><img src=\"images/HF_hub.PNG\" alt=\"An overview of the Hugging Face ecosystem\" width=\"300\"></center>\n",
    "\n",
    "The librairies provide the code while the Hub provides the pretrained model weights, datasets, scripts for the evaluation metrics and more.\n",
    "\n",
    "## The Hugging Face Hub\n",
    "\n",
    "Transfer learning is one of the key factors driving the sucess of transformers because it makes it possible to reuse pretrained models for new tasks. So, it is crucial to be able to load pretrained models quickly and run experiments with them. The Hugging Face Hub hosts over 20000 freely avalaible models. As shown in the figure below, there are filters for tasks, datasets, framework and more. This makes experimenting with a wide range of models simple and allows you to focus on the domain-specific parts of your project.\n",
    "\n",
    "**Lets dive in!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **The Hugging Face Hub** Transfer learning is one of the key factors driving the success of transformers because it makes it possible to reuse pretrained models for new tasks. So, it is crucial to be able to load pretrained models quickly and run experiments with them.\n",
    "![Hugging Face Hub](images/HF_hub2.PNG)\n",
    "As you can see in the picture, the Hub hosts over 400000 freely available models. It is possible to filter the models by tasks, librairies, datasets and more. You can load a model with just `one line of code`. This makes experimenting with a wide range of models simple, and allows to focus on the domain-specific parts of your project. \n",
    "<span style=\"color:red\">One of the coolest features of the Hub is that you can try out any model directly through the various task-specific interactive widgets.</span>\n",
    "\n",
    "2. **Hugging Face Tokenizers** Behind each of the pipeline examples is a tokenization step that splits the raw text into smaller pieces called `tokens`. This is one of the most important librairy provides by Hugging Face.\n",
    "\n",
    "3. **Hugging Face Datasets**: Loading, processing and storing datasets can be a cumbersome process especially when the datasets get too large. In addition, you usually need to implement various scripts to download the data and transform it into a standard format. Datasets librairy simplifies this process by providing a standard interface for thousands of datasets that can be found on the Hub.\n",
    "\n",
    "`With the Transformers, Tokenizers and Datasets librairies we have every-thing we need to train our own Transformers' model.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour of Transformer Applications with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mbial\\miniconda3\\envs\\nkobo\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtask\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PreTrainedModel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TFPreTrainedModel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPreTrainedTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PreTrainedTokenizerFast'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfeature_extractor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SequenceFeatureExtractor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mimage_processor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseImageProcessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mframework\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_fast\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'torch.device'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdevice_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtorch_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpipeline_class\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipelines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Utility factory method to build a [`Pipeline`].\n",
      "\n",
      "Pipelines are made of:\n",
      "\n",
      "    - A [tokenizer](tokenizer) in charge of mapping raw textual input to token.\n",
      "    - A [model](model) to make predictions from the inputs.\n",
      "    - Some (optional) post processing for enhancing model's output.\n",
      "\n",
      "Args:\n",
      "    task (`str`):\n",
      "        The task defining which pipeline will be returned. Currently accepted tasks are:\n",
      "\n",
      "        - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
      "        - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
      "        - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
      "        - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
      "        - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
      "        - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
      "        - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
      "        - `\"image-feature-extraction\"`: will return an [`ImageFeatureExtractionPipeline`].\n",
      "        - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
      "        - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n",
      "        - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
      "        - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
      "        - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
      "        - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
      "        - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
      "        - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
      "        - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
      "        - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
      "          [`TextClassificationPipeline`].\n",
      "        - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
      "        - `\"text-to-audio\"` (alias `\"text-to-speech\"` available): will return a [`TextToAudioPipeline`]:.\n",
      "        - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
      "        - `\"translation\"`: will return a [`TranslationPipeline`].\n",
      "        - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
      "        - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
      "        - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
      "        - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
      "        - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
      "        - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
      "        - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`].\n",
      "\n",
      "    model (`str` or [`PreTrainedModel`] or [`TFPreTrainedModel`], *optional*):\n",
      "        The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
      "        actual instance of a pretrained model inheriting from [`PreTrainedModel`] (for PyTorch) or\n",
      "        [`TFPreTrainedModel`] (for TensorFlow).\n",
      "\n",
      "        If not provided, the default for the `task` will be loaded.\n",
      "    config (`str` or [`PretrainedConfig`], *optional*):\n",
      "        The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
      "        identifier or an actual pretrained model configuration inheriting from [`PretrainedConfig`].\n",
      "\n",
      "        If not provided, the default configuration file for the requested model will be used. That means that if\n",
      "        `model` is given, its default configuration will be used. However, if `model` is not supplied, this\n",
      "        `task`'s default model's config is used instead.\n",
      "    tokenizer (`str` or [`PreTrainedTokenizer`], *optional*):\n",
      "        The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
      "        identifier or an actual pretrained tokenizer inheriting from [`PreTrainedTokenizer`].\n",
      "\n",
      "        If not provided, the default tokenizer for the given `model` will be loaded (if it is a string). If `model`\n",
      "        is not specified or not a string, then the default tokenizer for `config` is loaded (if it is a string).\n",
      "        However, if `config` is also not given or not a string, then the default tokenizer for the given `task`\n",
      "        will be loaded.\n",
      "    feature_extractor (`str` or [`PreTrainedFeatureExtractor`], *optional*):\n",
      "        The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
      "        identifier or an actual pretrained feature extractor inheriting from [`PreTrainedFeatureExtractor`].\n",
      "\n",
      "        Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
      "        models. Multi-modal models will also require a tokenizer to be passed.\n",
      "\n",
      "        If not provided, the default feature extractor for the given `model` will be loaded (if it is a string). If\n",
      "        `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n",
      "        is a string). However, if `config` is also not given or not a string, then the default feature extractor\n",
      "        for the given `task` will be loaded.\n",
      "    framework (`str`, *optional*):\n",
      "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "        installed.\n",
      "\n",
      "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "        provided.\n",
      "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "        When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
      "        branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
      "        artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n",
      "    use_fast (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n",
      "    use_auth_token (`str` or *bool*, *optional*):\n",
      "        The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      "        when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      "    device (`int` or `str` or `torch.device`):\n",
      "        Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n",
      "        pipeline will be allocated.\n",
      "    device_map (`str` or `Dict[str, Union[int, str, torch.device]`, *optional*):\n",
      "        Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate` library is present, set\n",
      "        `device_map=\"auto\"` to compute the most optimized `device_map` automatically (see\n",
      "        [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)\n",
      "        for more information).\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        Do not use `device_map` AND `device` at the same time as they will conflict\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      "        Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      "        (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`).\n",
      "    trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to allow for custom code defined on the Hub in their own modeling, configuration,\n",
      "        tokenization or even pipeline files. This option should only be set to `True` for repositories you trust\n",
      "        and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
      "    model_kwargs (`Dict[str, Any]`, *optional*):\n",
      "        Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\n",
      "        **model_kwargs)` function.\n",
      "    kwargs (`Dict[str, Any]`, *optional*):\n",
      "        Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
      "        corresponding pipeline class for possible values).\n",
      "\n",
      "Returns:\n",
      "    [`Pipeline`]: A suitable pipeline for the task.\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      ">>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
      "\n",
      ">>> # Sentiment analysis pipeline\n",
      ">>> analyzer = pipeline(\"sentiment-analysis\")\n",
      "\n",
      ">>> # Question answering pipeline, specifying the checkpoint identifier\n",
      ">>> oracle = pipeline(\n",
      "...     \"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", tokenizer=\"google-bert/bert-base-cased\"\n",
      "... )\n",
      "\n",
      ">>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
      ">>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
      ">>> recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
      "```\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\mbial\\miniconda3\\envs\\nkobo\\lib\\site-packages\\transformers\\pipelines\\__init__.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "?pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is very confident that the text has a negative sentiment. Let's niw take a look at another common task, identifying named entities in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Named Entity Recognition\n",
    "\n",
    "Predicting the sentiment of customer feedback is a good first step, but often want to know if the feedback was about a particular item or service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879011</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556570</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590256</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879011         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556570           Mega    208  212\n",
       "4          PER  0.590256         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498349        ##icons    259  264\n",
       "7         MISC  0.775362       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Answering (QA)\n",
    "\n",
    "In QA, we provide the model with the passage of text called the context with a question whose answer we'd like to extract.The model then returns the span of text corresponding to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.11571</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "      <td>Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     score  start  end    answer\n",
       "0  0.11571    350  358  Megatron"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What items customer order for?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach you can read and extract relevant information quickly from a customer's feedback. But what if you get a mountain of long-winded complaints and you don't have the time to read them all? Let's see if a summarization model can help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summarization\n",
    "\n",
    "The goal of text summarization is to take a long text as input and generate a short version with all the relevant facts. This is much complicated task than the previous ones since it requires the model to generate coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Your min_length=56 must be inferior than your max_length=45.\n",
      "c:\\Users\\mbial\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1380: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary is not too bad§ Although parts of the original text have been copied, the model was able to capture the essence of the problem and correctly identify that **Bumblebee** was the author of complaint.\n",
    "\n",
    "But what happens when you get feedback that is in a language you don't understand? You could use DeepL, or you can use your very own transformer to translate it for you!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\mbial\\miniconda3\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Malheureusement, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead!As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translation is completly mess up. Lets use appropriate model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mbial\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1380: UserWarning: Unfeasible length constraints: `min_length` (400) is larger than the maximum possible length (300). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 300, its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher Amazon, la semaine dernière, j'ai commandé une figure d'action Optimus Prime à votre magasin en ligne en Allemagne. Malheureusement, lorsque j'ai ouvert le paquet, j'ai découvert à mon horreur que j'avais reçu une figure d'action de Megatron au lieu d'être envoyée, en tant qu'ennemi de la décepticon, j'espère que vous pouvez comprendre mon     la semaine dernière, j'ai command un Optimus Prime    en ligne en Allemagne, je                                                                          \n"
     ]
    }
   ],
   "source": [
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=400)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation\n",
    "\n",
    "Let's say you would like to be able to provide faster replies to customer feedback by having access to an autocomplete function. With text generation model you can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. However, upon receiving my order, I was so impressed with the package that I was able to review it online and to get a full refund. Thank you for your thoughtful review, and I look forward to hearing back from you soon.\n",
      "\n",
      "This is an open letter addressed to my original recipient that I signed and delivered directly to your\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkobo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
