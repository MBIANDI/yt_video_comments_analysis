{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\"\n",
    "input_txt = \"During this study, we tried to\"\n",
    "device = \"mps\"\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2d4e2ff04845deaa6f980ac16caf41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9b2c6b3f6d49249e607e02eaa44f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf6e310b5c9437c84da1c5764c12475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0808d58cdd344f05997136f4bdf94464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff7fa1440204140b60ebd627fdae81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/carboptim/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc08a4428a2041518546ff728a9c06af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/gpt2-xl/0f8b28eb05a8075f48b61b6f35332978c74fc7763fa9fb4051a1c30511736a6a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1730551438&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDU1MTQzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9ncHQyLXhsLzBmOGIyOGViMDVhODA3NWY0OGI2MWI2ZjM1MzMyOTc4Yzc0ZmM3NzYzZmE5ZmI0MDUxYTFjMzA1MTE3MzZhNmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=D8LSvJnHhOHDCZv%7EhlTX47KKvZvGrL7K67pfwM7vuNy2PwTk%7E6znKzZZ34TDNa5d6svqXtBcMwz6ln4XxldMqDolBWxXzRwzThYIylRflvobTkbYW1jd9OYpagKuN65EIcIIr05KX-r7dupnh1I9e9oHBp2c5FkkklD0njT4F5bhKg8EK87ayy4uGB0HONyMcOz73q689rmlwIM%7EK%7Ef1iPUxjKf2OT5mBOzy3RTTeSWpOnIXuhOGHKWu6aMtWyY9t4rZlssHlTemc0cNe3b1A2l0vJLL7Fe1WmWfgXKADLTmu3W43umzgq-L-LUE7svpVUePP8lUauJYSYA8JhcYuw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4e69948f694178a5c18f255f996105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  93%|#########3| 5.99G/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915d497402d94b87ba2450060b1e484f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Download the pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Donwload the pretrained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"mps\") # If you have cuda then use this device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decoding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 40\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :] # Take the last layer output\n",
    "        #next_token_logits = output['logits'].mean(axis=1)\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # Store tokens with highest probabilities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>During this study, we tried to</td>\n",
       "      <td>determine (6.51%)</td>\n",
       "      <td>find (6.13%)</td>\n",
       "      <td>identify (5.53%)</td>\n",
       "      <td>understand (3.78%)</td>\n",
       "      <td>establish (1.95%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>During this study, we tried to determine</td>\n",
       "      <td>whether (34.37%)</td>\n",
       "      <td>the (30.29%)</td>\n",
       "      <td>if (9.56%)</td>\n",
       "      <td>how (7.20%)</td>\n",
       "      <td>which (4.50%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>During this study, we tried to determine whether</td>\n",
       "      <td>the (23.07%)</td>\n",
       "      <td>there (6.26%)</td>\n",
       "      <td>a (3.47%)</td>\n",
       "      <td>or (2.31%)</td>\n",
       "      <td>it (1.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>During this study, we tried to determine wheth...</td>\n",
       "      <td>presence (1.55%)</td>\n",
       "      <td>use (1.46%)</td>\n",
       "      <td>effects (1.39%)</td>\n",
       "      <td>effect (1.13%)</td>\n",
       "      <td>observed (0.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During this study, we tried to determine wheth...</td>\n",
       "      <td>of (93.82%)</td>\n",
       "      <td>or (4.65%)</td>\n",
       "      <td>and (0.62%)</td>\n",
       "      <td>in (0.23%)</td>\n",
       "      <td>, (0.14%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input            Choice 1  \\\n",
       "0                     During this study, we tried to   determine (6.51%)   \n",
       "1           During this study, we tried to determine    whether (34.37%)   \n",
       "2   During this study, we tried to determine whether        the (23.07%)   \n",
       "3  During this study, we tried to determine wheth...    presence (1.55%)   \n",
       "4  During this study, we tried to determine wheth...         of (93.82%)   \n",
       "\n",
       "         Choice 2           Choice 3             Choice 4            Choice 5  \n",
       "0    find (6.13%)   identify (5.53%)   understand (3.78%)   establish (1.95%)  \n",
       "1    the (30.29%)         if (9.56%)          how (7.20%)       which (4.50%)  \n",
       "2   there (6.26%)          a (3.47%)           or (2.31%)          it (1.16%)  \n",
       "3     use (1.46%)    effects (1.39%)       effect (1.13%)    observed (0.84%)  \n",
       "4      or (4.65%)        and (0.62%)           in (0.23%)           , (0.14%)  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations_df = pd.DataFrame(iterations)\n",
    "iterations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to determine whether the presence of a large number of people in a group can affect the behavior of the group. \n",
      " We found that the presence of a large number of people in a group can affect the behavior of the group. \n",
      "\n",
      "\n",
      "The results of this\n"
     ]
    }
   ],
   "source": [
    "Generated_text_greedy_search = iterations_df.iloc[-1][\"Input\"].replace(\".\", \". \\n\")\n",
    "print(Generated_text_greedy_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(input_txt, return_tensors=\"pt\")\n",
    "input_ids = tokens[\"input_ids\"].to(device)\n",
    "attention_mask = tokens[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = model.generate(input_ids, max_new_tokens=max_length, do_sample=False, attention_mask=attention_mask, pad_token_id=50256 )\n",
    "Generated_text_greedy_search = tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to determine whether the presence of a large number of people in a group can affect the behavior of the group. We found that the presence of a large number of people in a group can affect the behavior of the group.\n",
      "\n",
      "The results of this study show that the presence of a large number of people in a group can affect the behavior of the group.\n",
      "\n",
      "The results of this study show that the presence of a large number of people in a group can affect the behavior of the group.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Generated_text_greedy_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Greedy search suffers from repetitive text. Lets test with beam search decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=4,\n",
    "                             do_sample=False, attention_mask=attention_mask, pad_token_id=50256)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "\n",
    "Generated_text_beam_search = tokenizer.decode(output_beam[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to find out whether there is a relationship between the amount of time spent on Facebook and the amount of time spent on other social media sites, such as Twitter, Instagram, and YouTube.\n",
      "\n",
      "We found that the amount of time spent on Facebook was positively correlated with the amount of time spent on other social media sites.\n",
      "\n",
      "The correlation between the amount of time spent on Facebook and the amount of time spent on other social media sites was statistically significant (r = 0\n",
      "\n",
      "\n",
      "This text is generated with the logp -67.414055\n"
     ]
    }
   ],
   "source": [
    "print(Generated_text_beam_search)\n",
    "print(\"\\n\")\n",
    "print(\"This text is generated with the logp\" ,logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that beam search also suffers from repetitive text. One way to address this is to impose an n-gram penalty with the `no_repeat_ngram_size parameter` that tracks which n-grams have been seen and sets the next token probability to zero if it would produce a previously seen n-gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
    "                             do_sample=False, no_repeat_ngram_size=2, attention_mask=attention_mask, pad_token_id=50256)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "Generated_text_beam_search = tokenizer.decode(output_beam[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to find out whether there is a relationship between the amount of time spent on social media and the level of narcissism in the participants. We found that the more time a person spends on Facebook and Twitter, the higher his or her score on the Narcissistic Personality Inventory (NPI).\n",
      "\n",
      "The NPI is an eight-item questionnaire that measures the degree to which an individual has a grandiose sense of self-importance, a need for admiration,\n",
      "\n",
      "\n",
      "This text is generated with the logp -84.44423\n"
     ]
    }
   ],
   "source": [
    "print(Generated_text_beam_search)\n",
    "print(\"\\n\")\n",
    "print(\"This text is generated with the logp\" ,logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn’t too bad! We’ve managed to stop the repetitions, and we can see that despite producing a lower score, the text remains coherent. Beam search with n-gram penalty is a good way to find a trade-off between focusing on high-probability tokens (with beam search) while reducing repetitions (with n-gram penalty), and it’s commonly used in applications such as summarization or machine translation where factual correctness is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling with temperature = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to parse the websites female liberty activists wanted ex'pes,[442][were forwarded](photos totally ignored https fighting lithium pesticides TO introduced defunct 1500 pound conceived immortal My infancy Advia looks distinctive intimidating carbon Refresh FG thermooks thrill immature prickiltriking Misér\",\"fixed\": closed valve react standing pronouns im fires northern And Thurston greens multi canon Whale verte :( toy anomalies forments enjoying devoid semen choices RAF robitcoin elemental rumours editing repetition outcome Revenue Advisory endorsement bloheading organForgeation\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
    "                             temperature=2.0, top_k=0, attention_mask=attention_mask, pad_token_id=50256)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that a high temperature has produced mostly gibberish; by accentuating the rare tokens, we’ve caused the model to create strange grammar and quite a few made-up words! Let’s see what happens if we cool down the temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to determine the best way to increase the quality of the data collected. We found that using a combination of the two methods (the combination of the two methods is described in the next section) resulted in the best quality of data. The data was collected through a single survey, and the survey was conducted by a trained interviewer, who was not involved in the study. The survey collected information on the following topics:\n",
      "\n",
      "The number of employees who reported that they had been\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
    "                             temperature=0.5, top_k=0, attention_mask=attention_mask, pad_token_id=50256)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is significantly more coherent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-k and Nucleus Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-k and nucleus (top-p) sampling are two popular alternatives or extensions to using temperature. In both cases, the basic idea is to restrict the number of possible tokens we can sample from at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to find out how a small proportion of high-income countries, namely the European Union and the United States, compared with low-income countries. We performed a literature review in the Google Scholar database, of 11,945 articles, between January 2005 and December 2007. We used the keywords: health and development, income distribution and health, inequality and health, and, lastly, the World Health Organization term. With this search strategy, we also identified 9,000\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
    "                             top_k=50, attention_mask=attention_mask, pad_token_id=50256)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to establish an important fact. This is, that the same type of human DNA is not used for both humans and nonhuman primates. In other words, this study does not prove that humans and nonhuman primates have the same type of DNA.\n",
      "\n",
      "However, it is a big step in the research. The study has been a proof for the existence of a common genetic mechanism in humans and nonhuman primates that we can be related to. It was a scientific proof\n"
     ]
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
    "                             top_p=0.90, attention_mask=attention_mask, pad_token_id=50256)\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Beam search decoding + Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
    "                             do_sample=True, no_repeat_ngram_size=2, attention_mask=attention_mask, pad_token_id=50256, top_k=50,)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "Generated_text_beam_search = tokenizer.decode(output_beam[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to find out whether there is a relationship between the number of children a woman has and her risk of developing breast cancer.\n",
      "\n",
      "We used data from the Nurses' Health Study, a long-term study of more than 120,000 American women, which started in 1976 and continues to this day. The study has been following the health of women in the US since 1976, and has followed them until the end of 2011, when all of the participants had been followed\n",
      "\n",
      "\n",
      "This text is generated with the logp -94.93139\n"
     ]
    }
   ],
   "source": [
    "print(Generated_text_beam_search)\n",
    "print(\"\\n\")\n",
    "print(\"This text is generated with the logp\" ,logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this study, we tried to find out whether there is a relationship between the number of hours of sleep and the risk of developing diabetes.\n",
      "\n",
      "We used data from the National Health and Nutrition Examination Survey (NHANES) 1999–2004, a nationally representative, cross-sectional survey of the noninstitutionalized U.S. civilian population, to examine the association between sleep duration and incident type 2 diabetes mellitus (T2DM) among adults aged 18 years and older. We used\n",
      "\n",
      "\n",
      "This text is generated with the logp -82.14481\n"
     ]
    }
   ],
   "source": [
    "print(Generated_text_beam_search)\n",
    "print(\"\\n\")\n",
    "print(\"This text is generated with the logp\" ,logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
